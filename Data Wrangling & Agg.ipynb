{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triple A - Group Project\n",
    "## Predicting Taxi Demand in spatial and time resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workbook we combine all available data to get combined datasets for different temporal (1h, 2h, 6h, 12h) and spatial (Community Area, Census Tract) resolutions.\n",
    "Later, we will consider using h3 Uber Hexagon mapping across the city of Chicago top show why using a finer resolution in our eyes is not helpful with this specific data and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "import holidays\n",
    "import osmnx as ox\n",
    "import gc\n",
    "import folium\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIWFG\\AppData\\Local\\Temp\\ipykernel_24400\\4178553867.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  trips_data = pd.read_csv(\"data/Taxi_Trips__2024-__20250505.csv\", parse_dates = ['Trip Start Timestamp'])\n"
     ]
    }
   ],
   "source": [
    "ca_to_ct = pd.read_csv(\"data/CensusTractsTIGER2010_20250711.csv\", encoding='latin-1')\n",
    "ca_to_ct = ca_to_ct.rename(columns={'GEOID10': 'Tract', 'COMMAREA':'CommunityAreaNumber'})\n",
    "\n",
    "trips_data = pd.read_csv(\"data/Taxi_Trips__2024-__20250505.csv\", parse_dates = ['Trip Start Timestamp'])\n",
    "chi_weather = pd.read_csv(\"data/chicago_weather.csv\")\n",
    "poi = pd.read_csv(\"data/CHI_POI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading of Taxi Trips Data\n",
    "Since we consider demand, we will have to drop NAs of Pickp Community Areas. If we want to consider the finer resolution of Census Tracts, we have the option to <br>\n",
    "A) drop all rows missing the actual Census Tract or <br>\n",
    "B) infer a uniform distribution among Census Tracts from the available Community Area. <br>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Observations: 7917844\n",
      "Dropping 226777 Amount of Rows due to NaN Pickup Community Area\n",
      "7691067 Observations left\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Observations: {trips_data.shape[0]}')\n",
    "print(f'Dropping {trips_data['Pickup Community Area'].isna().sum()} Amount of Rows due to NaN Pickup Community Area')\n",
    "trips_data = trips_data.dropna(subset=['Pickup Community Area'])\n",
    "print(f'{trips_data.shape[0]} Observations left')\n",
    "\n",
    "trips_data['Trip Start Timestamp'] = pd.to_datetime(trips_data['Trip Start Timestamp'])\n",
    "trips_data['Trip Hour'] = trips_data['Trip Start Timestamp'].dt.floor('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do this, we need to check if the assignment of tracts to Cummunity Areas is right for all existing Tracts in our Trips Data. If so, we can infer the other Tracts to be assigned correctly in an informed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickup tracts missing in ca_to_ct mapping: []\n",
      "Found 0 pickup records where the CA doesn’t match:\n",
      "Empty DataFrame\n",
      "Columns: [Trip ID, Tract, trips_CA, map_CA, match]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips = trips_data.copy()\n",
    "mapping = ca_to_ct.copy()\n",
    "\n",
    "trips = trips[trips['Pickup Census Tract'].notna()].copy()\n",
    "mapping['CommunityAreaNumber'] = mapping['CommunityAreaNumber'].astype(int)\n",
    "\n",
    "trips['Pickup Census Tract'] = trips['Pickup Census Tract'].astype(int)\n",
    "trips['Pickup Community Area'] = trips['Pickup Community Area'].astype(int)\n",
    "\n",
    "# Find any pickup tracts in trips_data not in the mapping\n",
    "pickup_tracts = set(trips['Pickup Census Tract'].unique())\n",
    "mapped_tracts = set(mapping['Tract'].unique())\n",
    "\n",
    "missing_pickup = sorted(pickup_tracts - mapped_tracts)\n",
    "print(\"Pickup tracts missing in ca_to_ct mapping:\", missing_pickup)\n",
    "\n",
    "# For the ones that *are* in the mapping, check CA match\n",
    "pickup_check = (\n",
    "    trips[['Trip ID', 'Pickup Census Tract', 'Pickup Community Area']]\n",
    "    .rename(columns={\n",
    "        'Pickup Census Tract': 'Tract',\n",
    "        'Pickup Community Area': 'trips_CA'\n",
    "    })\n",
    "    .merge(\n",
    "        mapping[['Tract', 'CommunityAreaNumber']].rename(\n",
    "            columns={'CommunityAreaNumber': 'map_CA'}\n",
    "        ),\n",
    "        on='Tract',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "pickup_check['match'] = pickup_check['trips_CA'] == pickup_check['map_CA']\n",
    "\n",
    "mismatches = pickup_check[~pickup_check['match']]\n",
    "print(f\"Found {len(mismatches)} pickup records where the CA doesn’t match:\")\n",
    "print(mismatches.head())\n",
    "\n",
    "# Clean up\n",
    "del mapping, trips, mismatches\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIWFG\\AppData\\Local\\Temp\\ipykernel_24400\\2774956827.py:45: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  all_hours = pd.date_range(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips = trips_data.copy()\n",
    "mapping = ca_to_ct.copy()\n",
    "\n",
    "mapping['CommunityAreaNumber'] = mapping['CommunityAreaNumber'].astype(int)\n",
    "trips['Pickup Community Area'] = trips['Pickup Community Area'].astype(int)\n",
    "\n",
    "# Count tracts per community area\n",
    "tract_counts = (\n",
    "    mapping\n",
    "    .groupby('CommunityAreaNumber')['Tract']\n",
    "    .count()\n",
    "    .rename('tract_count')\n",
    "    .reset_index()\n",
    ")\n",
    "mapping = mapping.merge(tract_counts, on='CommunityAreaNumber')\n",
    "\n",
    "# Split known vs. unknown pickup tracts\n",
    "known = trips[trips['Pickup Census Tract'].notna()].copy()\n",
    "known['Tract'] = known['Pickup Census Tract'].astype(int)\n",
    "known['weight'] = 1.0\n",
    "\n",
    "unknown = trips[trips['Pickup Census Tract'].isna()].copy()\n",
    "unknown = unknown.merge(\n",
    "    mapping[['CommunityAreaNumber', 'Tract', 'tract_count']],\n",
    "    left_on='Pickup Community Area',\n",
    "    right_on='CommunityAreaNumber',\n",
    "    how='left'\n",
    ")\n",
    "unknown['weight'] = 1.0 / unknown['tract_count']\n",
    "\n",
    "# Concatenate and aggregate weighted counts\n",
    "expanded = pd.concat([\n",
    "    known[['Trip Hour', 'Tract', 'weight']],\n",
    "    unknown[['Trip Hour', 'Tract', 'weight']]\n",
    "], ignore_index=True)\n",
    "\n",
    "counts_by_hour_tract = (\n",
    "    expanded\n",
    "    .groupby(['Trip Hour', 'Tract'])['weight']\n",
    "    .sum()\n",
    "    .reset_index(name='trip_count')\n",
    ")\n",
    "\n",
    "# Build full cartesian index of all hours × all tracts\n",
    "all_hours = pd.date_range(\n",
    "    start=expanded['Trip Hour'].min(),\n",
    "    end=expanded['Trip Hour'].max(),\n",
    "    freq='H'\n",
    ")\n",
    "all_tracts = mapping['Tract'].unique()\n",
    "\n",
    "# Create MultiIndex and DataFrame\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [all_hours, all_tracts],\n",
    "    names=['Trip Hour', 'Tract']\n",
    ")\n",
    "full_index = pd.DataFrame(index=idx).reset_index()\n",
    "\n",
    "# Join the actual counts, fill missing as zero\n",
    "census_hourly  = (\n",
    "    full_index\n",
    "    .merge(counts_by_hour_tract, on=['Trip Hour', 'Tract'], how='left')\n",
    ")\n",
    "census_hourly['trip_count'] = census_hourly['trip_count'].fillna(0)\n",
    "\n",
    "# Clean up\n",
    "del mapping, trips\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the exact locational data, we have to enrich our dataframe with the Centroids Location from the original data. Since we disaggregate the missing Census Tracts from Community Areas, we have to calculate additional Centroids for the remaining NA coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching = trips_data.copy()\n",
    "\n",
    "matching = matching.rename(columns={'Pickup Census Tract': 'Tract'})\n",
    "merged_df = census_hourly.merge(\n",
    "    matching[['Tract', 'Pickup Centroid Location']].drop_duplicates(),\n",
    "    on='Tract',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "del matching\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ca_to_ct['the_geom'].dtype == 'O':\n",
    "    ca_to_ct['geometry'] = ca_to_ct['the_geom'].apply(wkt.loads)\n",
    "else:\n",
    "    ca_to_ct['geometry'] = ca_to_ct['the_geom']\n",
    "\n",
    "gdf = gpd.GeoDataFrame(ca_to_ct, geometry='geometry')\n",
    "\n",
    "\n",
    "gdf['centroid_point'] = gdf.geometry.centroid\n",
    "gdf['centroid_wkt'] = gdf['centroid_point'].apply(lambda p: p.wkt)\n",
    "\n",
    "\n",
    "tract_centroid_wkt = dict(zip(gdf['Tract'], gdf['centroid_wkt']))\n",
    "\n",
    "mask_na = merged_df['Pickup Centroid Location'].isna()\n",
    "merged_df.loc[mask_na, 'Pickup Centroid Location'] = merged_df.loc[mask_na, 'Tract'].map(tract_centroid_wkt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have all Lat/Lon data now, we want to infer h3 hexagons to capture the spatial information between the Tracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lat_lon(point_wkt):\n",
    "    \"\"\"\n",
    "    Takes WKT string like 'POINT (-87.6300448953 41.7424875717)'\n",
    "    Returns (lat, lon) as floats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove 'POINT (' and ')'\n",
    "        coords = point_wkt.replace('POINT (', '').replace(')', '').split()\n",
    "        lon, lat = map(float, coords)\n",
    "        return lat, lon\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def get_h3_index(row, res=9):\n",
    "    if pd.isnull(row['Pickup Centroid Location']):\n",
    "        return None\n",
    "    lat, lon = extract_lat_lon(row['Pickup Centroid Location'])\n",
    "    if lat is not None and lon is not None:\n",
    "        return h3.latlng_to_cell(lat, lon, res)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "merged_df['pickup_h3_9'] = merged_df.apply(get_h3_index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_hourly = merged_df.copy()\n",
    "\n",
    "del merged_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Chicago Weather Data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_desc(s):\n",
    "    try:\n",
    "        data = ast.literal_eval(s)\n",
    "\n",
    "        if isinstance(data, list) and data:\n",
    "            return data[0].get('description')\n",
    "\n",
    "        elif isinstance(data, dict):\n",
    "            return data.get('description')\n",
    "\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "#chi_weather['weather_description'] = chi_weather['weather'].apply(extract_desc) # algo can infer the weather description info from all other available data \n",
    "chi_weather['nighttime'] = chi_weather['pod'].apply(lambda x: 1 if x == 'n' else 0) # do we need? with sin/cos portraying algo can learn boundaries itself?\n",
    "chi_weather['timestamp_local'] = pd.to_datetime(chi_weather['timestamp_local'])\n",
    "chi_weather = chi_weather.drop(columns = ['weather', 'timestamp_utc', 'ts', \n",
    "                                            'datetime', 'date', 'slp', \n",
    "                                            'dhi', 'dni', 'ghi', \n",
    "                                            'solar_rad', 'azimuth', 'elev_angle', \n",
    "                                            'h_angle', 'revision_status', 'pod']) # dropping unnecessary columns, can think of also dropping temp since app_temp likely more influential\n",
    "chi_weather[['clouds', 'pres', 'rh', 'vis', 'wind_dir', 'nighttime']] = chi_weather[['clouds', 'pres', 'rh', 'vis', 'wind_dir', 'nighttime']].astype(float)\n",
    "\n",
    "#chi_weather.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two missing timestamps ['2024-03-10 02:00:00','2025-03-09 02:00:00'] for which we infer weather data from the previous and following timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ts = pd.to_datetime(['2024-03-10 02:00', '2025-03-09 02:00'])\n",
    "chi_weather = chi_weather.set_index('timestamp_local').sort_index()\n",
    "\n",
    "filled_rows = []\n",
    "for ts in missing_ts:\n",
    "    prev_h = ts - pd.Timedelta(hours=1)\n",
    "    next_h = ts + pd.Timedelta(hours=1)\n",
    "\n",
    "    row_before = chi_weather.loc[prev_h]\n",
    "    row_after  = chi_weather.loc[next_h]\n",
    "\n",
    "    inferred = (row_before + row_after) / 2\n",
    "    inferred.name = ts\n",
    "    filled_rows.append(inferred)\n",
    "\n",
    "df_filled = pd.DataFrame(filled_rows)\n",
    "\n",
    "chi_weather = pd.concat([chi_weather, df_filled]) \\\n",
    "                .sort_index() \\\n",
    "                .reset_index() \\\n",
    "                .rename(columns={'index':'timestamp_local'})\n",
    "\n",
    "\n",
    "#print(chi_weather[chi_weather['timestamp_local'] == '2024-03-10 02:00'])\n",
    "#print(chi_weather[chi_weather['timestamp_local'] == '2025-03-09 02:00'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIWFG\\AppData\\Local\\Temp\\ipykernel_24400\\1251629888.py:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  weather['Trip Hour'] = weather['timestamp_local'].dt.floor('H')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = chi_weather.copy()\n",
    "weather['Trip Hour'] = weather['timestamp_local'].dt.floor('H')\n",
    "\n",
    "\n",
    "weather_cols = [c for c in weather.columns \n",
    "                if c not in ('timestamp_local')]\n",
    "\n",
    "weather_hourly = (\n",
    "    weather[weather_cols]\n",
    "    .drop_duplicates(subset='Trip Hour', keep='first')\n",
    ")\n",
    "\n",
    "census_hourly = (\n",
    "    census_hourly\n",
    "    .merge(weather_hourly, on='Trip Hour', how='left')\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "del weather\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Match Locational Data from OpenStreetMap\n",
    "We count amenities (as of now: Restaurants, Cafes, Bars) per Census Tract to add to our Dataframes for prediction. This could be especially interesting when looking at demand in a more granular resolution than Census Tracts and Community Areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_gdf = poi.copy()\n",
    "tracts = ca_to_ct.copy()\n",
    "\n",
    "# parse string into wkt multipolygon object\n",
    "if poi_gdf.geometry.dtype == object:\n",
    "    poi_gdf['geometry'] = poi_gdf.geometry.apply(wkt.loads)\n",
    "poi_gdf = gpd.GeoDataFrame(poi_gdf, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "tracts['geometry'] = tracts['the_geom'].apply(wkt.loads)\n",
    "tracts = gpd.GeoDataFrame(\n",
    "    tracts, \n",
    "    geometry='geometry', \n",
    "    crs='EPSG:4326'\n",
    ")[['Tract', 'geometry']]\n",
    "\n",
    "# spatial‐join POIs into tracts\n",
    "#    “predicate='within'” ensures only points inside the polygon count\n",
    "joined = gpd.sjoin(\n",
    "    poi_gdf, \n",
    "    tracts, \n",
    "    how='inner', \n",
    "    predicate='within'\n",
    ")\n",
    "\n",
    "poi_counts = (\n",
    "    joined\n",
    "    .groupby('Tract')\n",
    "    .size()\n",
    "    .reset_index(name='poi_count')\n",
    ")\n",
    "\n",
    "# can also count by type (save for later eval)\n",
    "# poi_counts_by_type = (\n",
    "#     joined\n",
    "#     .groupby(['Tract','amenity'])\n",
    "#     .size()\n",
    "#     .unstack(fill_value=0)\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "\n",
    "census_hourly = (\n",
    "    census_hourly\n",
    "    .merge(poi_counts, on='Tract', how='left')\n",
    ")\n",
    "census_hourly['poi_count'] = census_hourly['poi_count'].fillna(0).astype(int)\n",
    "\n",
    "#census_hourly.head()\n",
    "\n",
    "# Clean up\n",
    "del poi_gdf, tracts, joined, poi_counts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Map public holidays to the final df (+ other date related features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import holidays\n",
    "\n",
    "us_holidays = holidays.US(state='IL', years=[2024, 2025])\n",
    "census_hourly['is_holiday'] = census_hourly['Trip Hour'].dt.date.apply(lambda x: int(x in us_holidays))\n",
    "\n",
    "#census_hourly.head(5)\n",
    "\n",
    "# Clean up\n",
    "del us_holidays\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekend Feature\n",
    "census_hourly['isWeekend'] = (census_hourly['Trip Hour'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Day, Hour and Month\n",
    "census_hourly['Trip Start Day'] = census_hourly['Trip Hour'].dt.day\n",
    "census_hourly['Trip Start Hour'] = census_hourly['Trip Hour'].dt.hour\n",
    "census_hourly['Trip Start Month'] = census_hourly['Trip Hour'].dt.month\n",
    "\n",
    "#census_hourly.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the df at this point to be able to access without having to run the whole notebook \n",
    "census_hourly.to_csv(\"data/census_hourly_h3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create remaining dfs\n",
    "We don't do anything new but just sample the hourly data into different temporal formats, summing the trips and using the mean for the remainig data points as the best guessed estimate of weather and all other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIWFG\\AppData\\Local\\Temp\\ipykernel_24400\\4273584323.py:16: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LIWFG\\AppData\\Local\\Temp\\ipykernel_24400\\4273584323.py:16: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n",
      "C:\\Users\\LIWFG\\AppData\\Local\\Temp\\ipykernel_24400\\4273584323.py:16: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n",
      "C:\\Users\\LIWFG\\AppData\\Local\\Temp\\ipykernel_24400\\4273584323.py:16: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n"
     ]
    }
   ],
   "source": [
    "df = census_hourly.copy()\n",
    "df_match = df[['pickup_h3_9', 'Tract']].drop_duplicates()\n",
    "df = df.drop(columns = ['pickup_h3_9', 'Pickup Centroid Location'])\n",
    "\n",
    "df['Trip Hour'] = pd.to_datetime(df['Trip Hour'])\n",
    "\n",
    "agg_dict = {\n",
    "    'trip_count': 'sum',\n",
    "    **{col: 'mean' for col in df.columns \n",
    "          if col not in ['trip_count', 'Tract', 'Trip Hour']}\n",
    "}\n",
    "\n",
    "def aggregate_by_freq(df, freq):\n",
    "    grouped = (\n",
    "        df.groupby(\n",
    "            [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n",
    "            observed=True\n",
    "        )\n",
    "        .agg(agg_dict)\n",
    "        .reset_index()\n",
    "    )\n",
    "    merged = grouped.merge(df_match, on='Tract', how='left')\n",
    "    return merged\n",
    "    \n",
    "\n",
    "census_2hourly  = aggregate_by_freq(df, '2H')\n",
    "census_4hourly  = aggregate_by_freq(df, '4H')\n",
    "census_6hourly  = aggregate_by_freq(df, '6H')\n",
    "census_12hourly = aggregate_by_freq(df, '12H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2hourly.to_csv(\"data/census_2hourly_h3.csv\", index=False)\n",
    "census_4hourly.to_csv(\"data/census_4hourly_h3.csv\", index=False)\n",
    "census_6hourly.to_csv(\"data/census_6hourly_h3.csv\", index=False)\n",
    "census_12hourly.to_csv(\"data/census_12hourly_h3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
