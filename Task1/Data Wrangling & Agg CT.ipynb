{"cells":[{"cell_type":"code","source":["pip install h3 osmnx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"B_T7rIQbcdX9","executionInfo":{"status":"ok","timestamp":1753217702602,"user_tz":-120,"elapsed":6470,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"81ba72e2-011a-42d5-eaef-cecde039268a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting h3\n","  Downloading h3-4.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting osmnx\n","  Downloading osmnx-2.0.5-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: geopandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from osmnx) (1.1.1)\n","Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from osmnx) (3.5)\n","Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.0.2)\n","Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.2.2)\n","Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.32.3)\n","Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.1.1)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0.1->osmnx) (0.11.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0.1->osmnx) (25.0)\n","Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from geopandas>=1.0.1->osmnx) (3.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (2025.7.14)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->osmnx) (1.17.0)\n","Downloading h3-4.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.8/985.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading osmnx-2.0.5-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.3/101.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h3, osmnx\n","Successfully installed h3-4.3.0 osmnx-2.0.5\n"]}]},{"cell_type":"markdown","metadata":{"id":"BDMnBcGAb7qP"},"source":["# Triple A - Group Project\n","## Predicting Taxi Demand in spatial and time resolution"]},{"cell_type":"markdown","metadata":{"id":"FQWD5M0Ab7qQ"},"source":["In this workbook we combine all available data to get combined datasets for different temporal (1h, 2h, 6h, 12h) and spatial (Community Area, Census Tract) resolutions.\n","Later, we will consider using h3 Uber Hexagon mapping across the city of Chicago top show why using a finer resolution in our eyes is not helpful with this specific data and use case."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CV05zqVrczIo","executionInfo":{"status":"ok","timestamp":1753217730257,"user_tz":-120,"elapsed":27634,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"66676f91-40ff-471b-b4ef-13e4e7bffeb9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RReCJ5gRb7qR","executionInfo":{"status":"ok","timestamp":1753217741151,"user_tz":-120,"elapsed":2898,"user":{"displayName":"C","userId":"04052112979947968003"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import ast\n","import h3\n","import geopandas as gpd\n","from shapely import wkt\n","from shapely.geometry import Polygon\n","import holidays\n","import osmnx as ox\n","import gc\n","import folium\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"kS7RmTRob7qR","executionInfo":{"status":"ok","timestamp":1753219045227,"user_tz":-120,"elapsed":101,"user":{"displayName":"C","userId":"04052112979947968003"}}},"outputs":[],"source":["ca_to_ct = pd.read_csv(\"/content/drive/MyDrive/Triple A/data/CensusTractsTIGER2010_20250711.csv\", encoding='latin-1')\n","ca_to_ct = ca_to_ct.rename(columns={'GEOID10': 'Tract', 'COMMAREA':'CommunityAreaNumber'})\n","\n","trips_data = pd.read_csv(\"/content/drive/MyDrive/Triple A/data/Taxi_Trips__2024-__20250711.csv\", parse_dates = ['Trip Start Timestamp'])\n","trips_data = trips_data[trips_data['Trip Start Timestamp'] < '2025-07-01 00:00:00']\n","\n","chi_weather = pd.read_csv(\"/content/drive/MyDrive/Triple A/data/chicago_weather.csv\")\n","poi = pd.read_csv(\"/content/drive/MyDrive/Triple A/data/CHI_POI.csv\")"]},{"cell_type":"markdown","metadata":{"id":"b2SBjT1Lb7qS"},"source":["#### Loading of Taxi Trips Data\n","Since we consider demand, we will have to drop NAs of Pickp Community Areas. If we want to consider the finer resolution of Census Tracts, we have the option to <br>\n","A) drop all rows missing the actual Census Tract or <br>\n","B) infer a uniform distribution among Census Tracts from the available Community Area. <br>\n",""]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhVz_BoMb7qS","executionInfo":{"status":"ok","timestamp":1753218832278,"user_tz":-120,"elapsed":2363,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"04aa1b1f-1f0f-4b21-dccf-23f0980a2348"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Observations: 9518152\n","Dropping 0 Amount of Rows due to NaN Pickup Community Area\n","9518152 Observations left\n"]}],"source":["print(f'Total Observations: {trips_data.shape[0]}')\n","print(f\"Dropping {trips_data['Pickup Community Area'].isna().sum()} Amount of Rows due to NaN Pickup Community Area\")\n","trips_data = trips_data.dropna(subset=['Pickup Community Area'])\n","print(f'{trips_data.shape[0]} Observations left')\n","\n","trips_data['Trip Start Timestamp'] = pd.to_datetime(trips_data['Trip Start Timestamp'])\n","trips_data['Trip Hour'] = trips_data['Trip Start Timestamp'].dt.floor('h')\n","\n","trips_data_ca = trips_data.copy()"]},{"cell_type":"markdown","metadata":{"id":"-wSB1sUbb7qS"},"source":["Before we do this, we need to check if the assignment of tracts to Cummunity Areas is right for all existing Tracts in our Trips Data. If so, we can infer the other Tracts to be assigned correctly in an informed manner."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSaUTR3Xb7qT","executionInfo":{"status":"ok","timestamp":1753218838776,"user_tz":-120,"elapsed":3444,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"c0314e2a-e003-496d-9de8-44ad91adb6cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pickup tracts missing in ca_to_ct mapping: []\n","Found 0 pickup records where the CA doesn’t match:\n","Empty DataFrame\n","Columns: [Trip ID, Tract, trips_CA, map_CA, match]\n","Index: []\n"]},{"output_type":"execute_result","data":{"text/plain":["21"]},"metadata":{},"execution_count":18}],"source":["trips = trips_data.copy()\n","mapping = ca_to_ct.copy()\n","\n","trips = trips[trips['Pickup Census Tract'].notna()].copy()\n","mapping['CommunityAreaNumber'] = mapping['CommunityAreaNumber'].astype(int)\n","\n","trips['Pickup Census Tract'] = trips['Pickup Census Tract'].astype(int)\n","trips['Pickup Community Area'] = trips['Pickup Community Area'].astype(int)\n","\n","# Find any pickup tracts in trips_data not in the mapping\n","pickup_tracts = set(trips['Pickup Census Tract'].unique())\n","mapped_tracts = set(mapping['Tract'].unique())\n","\n","missing_pickup = sorted(pickup_tracts - mapped_tracts)\n","print(\"Pickup tracts missing in ca_to_ct mapping:\", missing_pickup)\n","\n","# For the ones that *are* in the mapping, check CA match\n","pickup_check = (\n","    trips[['Trip ID', 'Pickup Census Tract', 'Pickup Community Area']]\n","    .rename(columns={\n","        'Pickup Census Tract': 'Tract',\n","        'Pickup Community Area': 'trips_CA'\n","    })\n","    .merge(\n","        mapping[['Tract', 'CommunityAreaNumber']].rename(\n","            columns={'CommunityAreaNumber': 'map_CA'}\n","        ),\n","        on='Tract',\n","        how='left'\n","    )\n",")\n","pickup_check['match'] = pickup_check['trips_CA'] == pickup_check['map_CA']\n","\n","mismatches = pickup_check[~pickup_check['match']]\n","print(f\"Found {len(mismatches)} pickup records where the CA doesn’t match:\")\n","print(mismatches.head())\n","\n","# Clean up\n","del mapping, trips, mismatches\n","gc.collect()"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3MxecCzb7qT","executionInfo":{"status":"ok","timestamp":1753218877011,"user_tz":-120,"elapsed":38214,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"2a0cb7f1-c984-4ba4-ad6a-a05933903301"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-19-1405071212.py:45: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n","  all_hours = pd.date_range(\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":19}],"source":["trips = trips_data.copy()\n","mapping = ca_to_ct.copy()\n","\n","mapping['CommunityAreaNumber'] = mapping['CommunityAreaNumber'].astype(int)\n","trips['Pickup Community Area'] = trips['Pickup Community Area'].astype(int)\n","\n","# Count tracts per community area\n","tract_counts = (\n","    mapping\n","    .groupby('CommunityAreaNumber')['Tract']\n","    .count()\n","    .rename('tract_count')\n","    .reset_index()\n",")\n","mapping = mapping.merge(tract_counts, on='CommunityAreaNumber')\n","\n","# Split known vs. unknown pickup tracts\n","known = trips[trips['Pickup Census Tract'].notna()].copy()\n","known['Tract'] = known['Pickup Census Tract'].astype(int)\n","known['weight'] = 1.0\n","\n","unknown = trips[trips['Pickup Census Tract'].isna()].copy()\n","unknown = unknown.merge(\n","    mapping[['CommunityAreaNumber', 'Tract', 'tract_count']],\n","    left_on='Pickup Community Area',\n","    right_on='CommunityAreaNumber',\n","    how='left'\n",")\n","unknown['weight'] = 1.0 / unknown['tract_count']\n","\n","# Concatenate and aggregate weighted counts\n","expanded = pd.concat([\n","    known[['Trip Hour', 'Tract', 'weight']],\n","    unknown[['Trip Hour', 'Tract', 'weight']]\n","], ignore_index=True)\n","\n","counts_by_hour_tract = (\n","    expanded\n","    .groupby(['Trip Hour', 'Tract'])['weight']\n","    .sum()\n","    .reset_index(name='trip_count')\n",")\n","\n","# Build full cartesian index of all hours × all tracts\n","all_hours = pd.date_range(\n","    start=expanded['Trip Hour'].min(),\n","    end=expanded['Trip Hour'].max(),\n","    freq='H'\n",")\n","all_tracts = mapping['Tract'].unique()\n","\n","# Create MultiIndex and DataFrame\n","idx = pd.MultiIndex.from_product(\n","    [all_hours, all_tracts],\n","    names=['Trip Hour', 'Tract']\n",")\n","full_index = pd.DataFrame(index=idx).reset_index()\n","\n","# Join the actual counts, fill missing as zero\n","census_hourly  = (\n","    full_index\n","    .merge(counts_by_hour_tract, on=['Trip Hour', 'Tract'], how='left')\n",")\n","census_hourly['trip_count'] = census_hourly['trip_count'].fillna(0)\n","\n","# Clean up\n","del mapping, trips, known, unknown, tract_counts, counts_by_hour_tract, all_hours\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"KllKsWQLb7qT"},"source":["To get the exact locational data, we have to enrich our dataframe with the Centroids Location from the original data. Since we disaggregate the missing Census Tracts from Community Areas, we have to calculate additional Centroids for the remaining NA coordinates."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koJwt4YIb7qT","executionInfo":{"status":"ok","timestamp":1753218882528,"user_tz":-120,"elapsed":5504,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"7e7ef63d-5dc8-4ead-90a8-d094e81e5a5a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":20}],"source":["matching = trips_data.copy()\n","\n","matching = matching.rename(columns={'Pickup Census Tract': 'Tract'})\n","merged_df = census_hourly.merge(\n","    matching[['Tract', 'Pickup Centroid Location']].drop_duplicates(),\n","    on='Tract',\n","    how='left'\n",")\n","\n","del matching\n","gc.collect()"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"R67_Zh4jb7qT","executionInfo":{"status":"ok","timestamp":1753218883470,"user_tz":-120,"elapsed":829,"user":{"displayName":"C","userId":"04052112979947968003"}}},"outputs":[],"source":["if ca_to_ct['the_geom'].dtype == 'O':\n","    ca_to_ct['geometry'] = ca_to_ct['the_geom'].apply(wkt.loads)\n","else:\n","    ca_to_ct['geometry'] = ca_to_ct['the_geom']\n","\n","gdf = gpd.GeoDataFrame(ca_to_ct, geometry='geometry')\n","\n","\n","gdf['centroid_point'] = gdf.geometry.centroid\n","gdf['centroid_wkt'] = gdf['centroid_point'].apply(lambda p: p.wkt)\n","\n","\n","tract_centroid_wkt = dict(zip(gdf['Tract'], gdf['centroid_wkt']))\n","\n","mask_na = merged_df['Pickup Centroid Location'].isna()\n","merged_df.loc[mask_na, 'Pickup Centroid Location'] = merged_df.loc[mask_na, 'Tract'].map(tract_centroid_wkt)\n"]},{"cell_type":"markdown","metadata":{"id":"QyUieFnxb7qT"},"source":["Since we have all Lat/Lon data now, we want to infer h3 hexagons to capture the spatial information between the Tracts."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCf3Oi_Ab7qT","executionInfo":{"status":"ok","timestamp":1753219005347,"user_tz":-120,"elapsed":121695,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"a0b3feb1-8996-41f6-affa-f72b0d27d461"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":22}],"source":["def extract_lat_lon(point_wkt):\n","    \"\"\"\n","    Takes WKT string like 'POINT (-87.6300448953 41.7424875717)'\n","    Returns (lat, lon) as floats.\n","    \"\"\"\n","    try:\n","        # Remove 'POINT (' and ')'\n","        coords = point_wkt.replace('POINT (', '').replace(')', '').split()\n","        lon, lat = map(float, coords)\n","        return lat, lon\n","    except:\n","        return None, None\n","\n","def get_h3_index(row, res=9):\n","    if pd.isnull(row['Pickup Centroid Location']):\n","        return None\n","    lat, lon = extract_lat_lon(row['Pickup Centroid Location'])\n","    if lat is not None and lon is not None:\n","        return h3.latlng_to_cell(lat, lon, res)\n","    else:\n","        return None\n","\n","\n","merged_df['pickup_h3_9'] = merged_df.apply(get_h3_index, axis=1)\n","census_hourly = merged_df.copy()\n","\n","del merged_df\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"yhNEsr6Mb7qU"},"source":["### Adding Chicago Weather Data from API"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"KkWHoltjb7qU","executionInfo":{"status":"ok","timestamp":1753219055839,"user_tz":-120,"elapsed":10,"user":{"displayName":"C","userId":"04052112979947968003"}}},"outputs":[],"source":["def extract_desc(s):\n","    try:\n","        data = ast.literal_eval(s)\n","\n","        if isinstance(data, list) and data:\n","            return data[0].get('description')\n","\n","        elif isinstance(data, dict):\n","            return data.get('description')\n","\n","    except (ValueError, SyntaxError):\n","        pass\n","\n","    return None\n","\n","#chi_weather['weather_description'] = chi_weather['weather'].apply(extract_desc) # algo can infer the weather description info from all other available data\n","chi_weather['nighttime'] = chi_weather['pod'].apply(lambda x: 1 if x == 'n' else 0) # do we need? with sin/cos portraying algo can learn boundaries itself?\n","chi_weather['timestamp_local'] = pd.to_datetime(chi_weather['timestamp_local'])\n","chi_weather = chi_weather.drop(columns = ['weather', 'timestamp_utc', 'ts',\n","                                            'datetime', 'date', 'slp',\n","                                            'dhi', 'dni', 'ghi',\n","                                            'solar_rad', 'azimuth', 'elev_angle',\n","                                            'h_angle', 'revision_status', 'pod']) # dropping unnecessary columns, can think of also dropping temp since app_temp likely more influential\n","chi_weather[['clouds', 'pres', 'rh', 'vis', 'wind_dir', 'nighttime']] = chi_weather[['clouds', 'pres', 'rh', 'vis', 'wind_dir', 'nighttime']].astype(float)\n","\n","#chi_weather.head(5)"]},{"cell_type":"markdown","metadata":{"id":"n0ZogPf5b7qU"},"source":["We have two missing timestamps ['2024-03-10 02:00:00','2025-03-09 02:00:00'] for which we infer weather data from the previous and following timestamps."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"YGRUCH-Rb7qU","executionInfo":{"status":"ok","timestamp":1753219058588,"user_tz":-120,"elapsed":21,"user":{"displayName":"C","userId":"04052112979947968003"}}},"outputs":[],"source":["missing_ts = pd.to_datetime(['2024-03-10 02:00', '2025-03-09 02:00'])\n","chi_weather = chi_weather.set_index('timestamp_local').sort_index()\n","\n","filled_rows = []\n","for ts in missing_ts:\n","    prev_h = ts - pd.Timedelta(hours=1)\n","    next_h = ts + pd.Timedelta(hours=1)\n","\n","    row_before = chi_weather.loc[prev_h]\n","    row_after  = chi_weather.loc[next_h]\n","\n","    inferred = (row_before + row_after) / 2\n","    inferred.name = ts\n","    filled_rows.append(inferred)\n","\n","df_filled = pd.DataFrame(filled_rows)\n","\n","chi_weather = pd.concat([chi_weather, df_filled]) \\\n","                .sort_index() \\\n","                .reset_index() \\\n","                .rename(columns={'index':'timestamp_local'})\n","\n","\n","#print(chi_weather[chi_weather['timestamp_local'] == '2024-03-10 02:00'])\n","#print(chi_weather[chi_weather['timestamp_local'] == '2025-03-09 02:00'])"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zmvvS40b7qU","executionInfo":{"status":"ok","timestamp":1753219061331,"user_tz":-120,"elapsed":1320,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"ada76173-5a10-45c0-ccdc-958133c6a319"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-27-1251629888.py:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n","  weather['Trip Hour'] = weather['timestamp_local'].dt.floor('H')\n"]},{"output_type":"execute_result","data":{"text/plain":["118"]},"metadata":{},"execution_count":27}],"source":["weather = chi_weather.copy()\n","weather['Trip Hour'] = weather['timestamp_local'].dt.floor('H')\n","\n","\n","weather_cols = [c for c in weather.columns\n","                if c not in ('timestamp_local')]\n","\n","weather_hourly = (\n","    weather[weather_cols]\n","    .drop_duplicates(subset='Trip Hour', keep='first')\n",")\n","\n","census_hourly = (\n","    census_hourly\n","    .merge(weather_hourly, on='Trip Hour', how='left')\n",")\n","\n","# Clean up\n","del weather\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"4KKzEHdWb7qU"},"source":["---\n","### Match Locational Data from OpenStreetMap\n","We count amenities (as of now: Restaurants, Cafes, Bars) per Census Tract to add to our Dataframes for prediction. This could be especially interesting when looking at demand in a more granular resolution than Census Tracts and Community Areas."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sg65Kp_ub7qU","executionInfo":{"status":"ok","timestamp":1753219071847,"user_tz":-120,"elapsed":2606,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"c5ba8dda-a30d-4e81-d34d-1d209d699b0b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["21"]},"metadata":{},"execution_count":29}],"source":["poi_gdf = poi.copy()\n","tracts = ca_to_ct.copy()\n","\n","# parse string into wkt multipolygon object\n","if poi_gdf.geometry.dtype == object:\n","    poi_gdf['geometry'] = poi_gdf.geometry.apply(wkt.loads)\n","poi_gdf = gpd.GeoDataFrame(poi_gdf, geometry='geometry', crs='EPSG:4326')\n","\n","tracts['geometry'] = tracts['the_geom'].apply(wkt.loads)\n","tracts = gpd.GeoDataFrame(\n","    tracts,\n","    geometry='geometry',\n","    crs='EPSG:4326'\n",")[['Tract', 'geometry']]\n","\n","# spatial‐join POIs into tracts\n","joined = gpd.sjoin(\n","    poi_gdf,\n","    tracts,\n","    how='inner',\n","    predicate='within'\n",")\n","\n","poi_counts = (\n","    joined\n","    .groupby('Tract')\n","    .size()\n","    .reset_index(name='poi_count')\n",")\n","\n","# can also count by type (save for later eval)\n","#poi_counts_by_type = (\n","#    joined\n","#    .groupby(['Tract','amenity'])\n","#    .size()\n","#    .unstack(fill_value=0)\n","#    .reset_index()\n","#)\n","\n","\n","census_hourly = (\n","    census_hourly\n","    .merge(poi_counts, on='Tract', how='left')\n",")\n","census_hourly['poi_count'] = census_hourly['poi_count'].fillna(0).astype(int)\n","\n","\n","# Clean up\n","del poi_gdf, tracts, joined, poi_counts\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"Nl1Zuokrb7qU"},"source":["---\n","### Map public holidays to the final df (+ other date related features)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bISb-Nd4b7qU","executionInfo":{"status":"ok","timestamp":1753219083767,"user_tz":-120,"elapsed":11892,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"33c1a140-50b6-4c21-905d-022d55a36797"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":30}],"source":["import holidays\n","\n","us_holidays = holidays.US(state='IL', years=[2024, 2025])\n","census_hourly['is_holiday'] = census_hourly['Trip Hour'].dt.date.apply(lambda x: int(x in us_holidays))\n","\n","# Clean up\n","del us_holidays\n","gc.collect()"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"H1s_BvINb7qU","executionInfo":{"status":"ok","timestamp":1753219084744,"user_tz":-120,"elapsed":965,"user":{"displayName":"C","userId":"04052112979947968003"}}},"outputs":[],"source":["# Weekend Feature\n","census_hourly['isWeekend'] = (census_hourly['Trip Hour'].dt.dayofweek >= 5).astype(int)\n","\n","# Day, Hour and Month\n","census_hourly['Trip Start Day'] = census_hourly['Trip Hour'].dt.day\n","census_hourly['Trip Start Hour'] = census_hourly['Trip Hour'].dt.hour\n","census_hourly['Trip Start Month'] = census_hourly['Trip Hour'].dt.month"]},{"cell_type":"code","source":["# Sine and Cosine Transformations\n","# Hour of Day\n","census_hourly[\"Trip Start Hour Sin\"] = np.sin(2 * np.pi * census_hourly[\"Trip Start Hour\"] / 24)\n","census_hourly[\"Trip Start Hour Cos\"] = np.cos(2 * np.pi * census_hourly[\"Trip Start Hour\"] / 24)\n","\n","\n","# Day of the Week\n","census_hourly[\"Trip Start Day Sin\"] = np.sin(2 * np.pi * census_hourly[\"Trip Start Day\"] / 7)\n","census_hourly[\"Trip Start Day Cos\"] = np.cos(2 * np.pi * census_hourly[\"Trip Start Day\"] / 7)\n","\n","\n","# Month of the Year\n","census_hourly[\"Trip Start Month Sin\"] = np.sin(2 * np.pi * census_hourly[\"Trip Start Month\"] / 12)\n","census_hourly[\"Trip Start Month Cos\"] = np.cos(2 * np.pi * census_hourly[\"Trip Start Month\"] / 12)\n"],"metadata":{"id":"qjfbQcCCvTdb","executionInfo":{"status":"ok","timestamp":1753219085974,"user_tz":-120,"elapsed":1213,"user":{"displayName":"C","userId":"04052112979947968003"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"id":"5M2MQuD-b7qU","executionInfo":{"status":"ok","timestamp":1753219396205,"user_tz":-120,"elapsed":310228,"user":{"displayName":"C","userId":"04052112979947968003"}}},"outputs":[],"source":["census_hourly.to_csv(\"/content/drive/MyDrive/Triple A/data/census_hourly_h3.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"qd8Nx-hsb7qU"},"source":["---\n","### Create remaining dfs\n","We don't do anything new but just sample the hourly data into different temporal formats, summing the trips and using the mean for the remainig data points as the best guessed estimate of weather and all other features."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcPxHcC4b7qU","executionInfo":{"status":"ok","timestamp":1753219422599,"user_tz":-120,"elapsed":26388,"user":{"displayName":"C","userId":"04052112979947968003"}},"outputId":"abd85e0e-d918-48b3-c7a4-a832442b5842"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-34-2782268005.py:17: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n","  [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n","/tmp/ipython-input-34-2782268005.py:17: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n","  [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n","/tmp/ipython-input-34-2782268005.py:17: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n","  [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n","/tmp/ipython-input-34-2782268005.py:17: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n","  [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n"]}],"source":["df = census_hourly.copy()\n","\n","df_match = df[['pickup_h3_9', 'Tract']].drop_duplicates()\n","df = df.drop(columns = ['pickup_h3_9', 'Pickup Centroid Location'])\n","\n","df['Trip Hour'] = pd.to_datetime(df['Trip Hour'])\n","\n","agg_dict = {\n","    'trip_count': 'sum',\n","    **{col: 'mean' for col in df.columns\n","          if col not in ['trip_count', 'Tract', 'Trip Hour']}\n","}\n","\n","def aggregate_by_freq(df, freq):\n","    grouped = (\n","        df.groupby(\n","            [pd.Grouper(key='Trip Hour', freq=freq), 'Tract'],\n","            observed=True\n","        )\n","        .agg(agg_dict)\n","        .reset_index()\n","    )\n","    merged = grouped.merge(df_match, on='Tract', how='left')\n","    return merged\n","\n","\n","census_2hourly  = aggregate_by_freq(df, '2H')\n","census_4hourly  = aggregate_by_freq(df, '4H')\n","census_6hourly  = aggregate_by_freq(df, '6H')\n","census_12hourly = aggregate_by_freq(df, '12H')\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"NwFL_pE1b7qV","executionInfo":{"status":"ok","timestamp":1753219873330,"user_tz":-120,"elapsed":450724,"user":{"displayName":"C","userId":"04052112979947968003"}}},"outputs":[],"source":["census_2hourly.to_csv(\"/content/drive/MyDrive/Triple A/data/census_2hourly_h3.csv\", index=False)\n","census_4hourly.to_csv(\"/content/drive/MyDrive/Triple A/data/census_4hourly_h3.csv\", index=False)\n","census_6hourly.to_csv(\"/content/drive/MyDrive/Triple A/data/census_6hourly_h3.csv\", index=False)\n","census_12hourly.to_csv(\"/content/drive/MyDrive/Triple A/data/census_12hourly_h3.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}